{"cells":[{"cell_type":"markdown","source":["#Scalable scoring with Databricks pipeline"],"metadata":{}},{"cell_type":"markdown","source":["#### After training and selecting best model, it's now time to run inferencing on large datasets. This notebook introduce a method where you can setup a scoring pipeline to run your model against large dataset using Spark distributed processing framework"],"metadata":{}},{"cell_type":"code","source":["# import the Workspace class and check the azureml SDK version\n# exist_ok checks if workspace exists or not.\nfrom azureml.core import Workspace\nfrom azureml.core.authentication import InteractiveLoginAuthentication\nsubscription_id = dbutils.secrets.get(\"commonakv\", \"subscriptionid\") #you should be owner or contributor\nresource_group = \"hcnDthCommon\" #you should be owner or contributor\n\nworkspace_name = \"amlcommonws\" #your workspace name\n\nworkspace_region = \"westus2\" #your region\nws = Workspace.create(name = workspace_name,\n                      subscription_id = subscription_id,\n                      auth = InteractiveLoginAuthentication(force=True, tenant_id=dbutils.secrets.get(\"commonakv\", \"tenantid\")),\n                      resource_group = resource_group, \n                      location = workspace_region,\n                      \n                      exist_ok=True)\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Performing interactive authentication. Please follow the instructions on the terminal.\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code B8YRRMNXU to authenticate.\nInteractive authentication successfully completed.\n</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["## 1. Load spark mlib model and score"],"metadata":{}},{"cell_type":"markdown","source":["It's straight forward to score against a model trained and stored as spark ml"],"metadata":{}},{"cell_type":"code","source":["#Download the model from the best run to a local folder. Use this during inference. \n\nimport os\nfrom azureml.core.model import Model\nimport shutil\nfrom pyspark.ml import Pipeline, PipelineModel\n\nmodel_name = \"crime_prediction_RDF.mml\"\n#spark ml can only load model from hdfs/dbfs file, not local file\nmodel_name_dbfs_client_path = \"/dbfs/mnt/models/\"\nmodel_name_dbfs_server_path = \"dbfs:/mnt/models/\"\n#Initialize model and loading from Azure ML using latest version\nmodel = Model(name = model_name,workspace = ws)\nif os.path.isfile(model_name) or os.path.isdir(model_name):\n    shutil.rmtree(model_name)\n\nmodel.download(model_name_dbfs_client_path, exist_ok=True)\ndbutils.fs.cp(\"file:\"+model_name_dbfs_client_path,model_name_dbfs_server_path, True)\n\nmodel_p_best = PipelineModel.load(model_name_dbfs_server_path+\"/\"+model_name)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#score the result\ncrime_df = spark.sql(\"select * from crime_dataset\").na.drop()\nresult = model_p_best.transform(crime_df)\nresult.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"spark_mlprediction\")\ndisplay(spark.sql(\"select * from spark_mlprediction\"))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## 2. Load SKLearn model (or ternsorflow etc..) and score"],"metadata":{}},{"cell_type":"markdown","source":["## If you trained your model with SKLearn or Tensorflow (not using Pyspark), it's still possible to run scalable scoring against large dataset. The approach below uses Pandas UDF to score batches of data from loaded SKLearn model you trained in prediction notebook"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import DoubleType\nimport pandas as pd\nfrom sklearn.externals import joblib\n\n\ndef make_predictions(sc, df, feature_cols, model_path, tf_path):\n    \"\"\"\n    Make predictions.\n    \n    Arguments:\n        sc: SparkContext.\n        df (pyspark.sql.DataFrame): Input data frame containing feature_cols.\n        feature_cols (list[str]): List of feature columns in df.\n        model_path (str): Path to model on Spark driver\n\n    Returns:\n        df (pyspark.sql.DataFrame): Output data frame with probability column.\n    \"\"\"\n    # Load classifier and broadcast to executors.\n    model = sc.broadcast(joblib.load(model_path))\n    tf = sc.broadcast(joblib.load(tf_path))\n\n    # Define Pandas UDF\n    @F.pandas_udf(returnType=DoubleType(), functionType=F.PandasUDFType.SCALAR)\n    def predict(*cols):\n        # Columns are passed as a tuple of Pandas Series'.\n        # Combine into a Pandas DataFrame\n        pdCrime = pd.concat(cols, axis=1)\n        pdCrime.columns = feature_cols\n        X = tf.value.transform(pdCrime)\n\n#         # Make predictions .\n        predictions = model.value.predict(X)\n        # Return Pandas Series of predictions.\n        return pd.Series(predictions)\n\n    # Make predictions on Spark DataFrame.\n    df = df.withColumn(\"predictions\", predict(*feature_cols))\n    \n    return df"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#Download the model from the best run to a local folder. Use this during inference. \n\nimport os\nfrom azureml.core.model import Model\nimport shutil\nfrom sklearn.externals import joblib\n\nfrom pyspark.ml import Pipeline, PipelineModel\n#Initialize model and loading from Azure ML using latest version\n\nmodel_name = \"ml.joblib\"\n#spark ml can only load model from hdfs/dbfs file, not local file\nmodel_name_dbfs_client_path = \"/dbfs/mnt/models/\"+model_name\n\nft_file = 'ft.joblib'\nml_file = 'ml.joblib'\n\nft_path = os.path.join(model_name_dbfs_client_path+\"/\"+model_name,ft_file )\nml_path = os.path.join(model_name_dbfs_client_path+\"/\"+model_name,ml_file )\n\nmodel = Model(name = model_name,workspace = ws)\nif os.path.isfile(model_name) or os.path.isdir(model_name):\n    shutil.rmtree(model_name)\n\nmodel.download(model_name_dbfs_client_path, exist_ok=True)\ncrime_df = spark.sql(\"select * from crime_dataset\").na.drop()\nfeature_cols = ['week', 'day', 'district', 'primary_type', 'school_test_performance',\n       'population', 'Unemployment_Rte', 'Median_Household_Income',\n       'Average_Commute_Time', 'Area', 'PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN',\n       'TOBS', 'WT01', 'WT03', 'WT04', 'WT05', 'WT06', 'WT11']\nsc = spark.sparkContext\n\npred=  make_predictions(sc, crime_df, feature_cols, ml_path, ft_path)\npred.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sk_learn_prediction\")\ndisplay(spark.sql(\"select * from sk_learn_prediction\"))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## 3. Scoring with AKS deployed services"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import DoubleType\nimport pandas as pd\nimport json\nimport requests\nimport ast\nimport time\ndef make_predictions_aks(sc, df, feature_cols):\n    \"\"\"\n    Make predictions.\n    \n    Arguments:\n        sc: SparkContext.\n        df (pyspark.sql.DataFrame): Input data frame containing feature_cols.\n        feature_cols (list[str]): List of feature columns in df.\n\n    Returns:\n        df (pyspark.sql.DataFrame): Output data frame with probability column.\n    \"\"\"\n    scoring_uri ='http://13.66.160.122/api/v1/service/crime-pred-service-1/score'\n    api_key ='i4J6SsvDPI3XuifhtI9NIhxrYGtt2Caz'\n    # Define Pandas UDF\n    @F.pandas_udf(returnType=DoubleType(), functionType=F.PandasUDFType.SCALAR)\n    def predict(*cols):\n        # Columns are passed as a tuple of Pandas Series'.\n        # Combine into a Pandas DataFrame\n        pdCrime = pd.concat(cols, axis=1)\n        pdCrime.columns = feature_cols\n        json_raw = json.dumps(pdCrime.to_json(orient='split'))\n        headers = {'Content-Type':'application/json',  'Authorization':('Bearer '+ api_key)} \n        \n        \n        response = requests.post(scoring_uri, data=json_raw, headers=headers)\n        while response.status_code != 200:\n          time.sleep(1)\n          response = requests.post(scoring_uri, data=json_raw, headers=headers)\n\n\n#         # Make predictions .\n        predictions_json = json.loads(response.content.decode(\"utf-8\"))\n        pred_list =ast.literal_eval(predictions_json)\n        predictions =pd.Series(pred_list)\n        # Return Pandas Series of predictions.\n        return predictions\n\n    # Make predictions on Spark DataFrame.\n    df = df.withColumn(\"predictions\", predict(*feature_cols))\n    \n    return df"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["\ncrime_df = spark.sql(\"select * from crime_dataset\").na.drop()\nfeature_cols = ['week', 'day', 'district', 'primary_type', 'school_test_performance',\n       'population', 'Unemployment_Rte', 'Median_Household_Income',\n       'Average_Commute_Time', 'Area', 'PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN',\n       'TOBS', 'WT01', 'WT03', 'WT04', 'WT05', 'WT06', 'WT11']\nsc = spark.sparkContext\n\npred=  make_predictions_aks(sc, crime_df, feature_cols)\npred.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"aks_prediction\")\ndisplay(spark.sql(\"select * from aks_prediction\"))\n# display(pred)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["#Write result to SQL DW Table"],"metadata":{}},{"cell_type":"code","source":["#Using High PErformance Databricks SQL DW driver. STep 1 is to setup spark conf with intermediary blob credentials\nstorage_account = \"i360pocstorage\"\ncontainer= \"sqldwdatabricks\"\nspark.conf.set(\n  \"fs.azure.account.key.\"+storage_account+\".blob.core.windows.net\",dbutils.secrets.get(\"commonakv\", \"i360pocstoragekey\"))\nspark.conf.set(\"spark.network.timeout\",\"10000000\")\n#Step 2: fill in SQL DW credentials, in real development, use secret scope insftead\nusername = dbutils.secrets.get(\"commonakv\", \"sqldwusername\")\n\npassword = dbutils.secrets.get(\"commonakv\", \"sqldwpassword\")\nservername = dbutils.secrets.get(\"commonakv\", \"sqldwserver\")\ndatabase = dbutils.secrets.get(\"commonakv\", \"sqldwdb\")\n\ntempdir = \"wasbs://\"+container+\"@\"+storage_account+\".blob.core.windows.net/\"\nconnection_string = \"jdbc:sqlserver://{0}.database.windows.net:1433;database={1};user={2}@{3};password={4}\".format(servername, database,username,servername,password,servername)\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#Read data from the table you stored prediction result in previous step into a dataframe\nsql_sk_pred_df = spark.sql(\"select * from sk_learn_prediction\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#Write out to a table in SQL DW\nsql_sk_pred_df.write \\\n  .format(\"com.databricks.spark.sqldw\") \\\n  .option(\"url\", connection_string) \\\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n  .option(\"dbTable\", \"dbo.sk_learn_prediction\") \\\n  .option(\"tempDir\", tempdir) \\\n  .mode(\"overwrite\") \\\n  .save()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["#You can also read from SQL DW\n\npred_out = spark.read \\\n  .format(\"com.databricks.spark.sqldw\") \\\n  .option(\"url\", connection_string) \\\n  .option(\"tempDir\",tempdir ) \\\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n  .option(\"dbTable\", \"dbo.sk_learn_prediction\") \\\n  .load()\ndisplay(pred_out)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":20}],"metadata":{"name":"Chicago-Crime-Scoring","notebookId":1363351547788863},"nbformat":4,"nbformat_minor":0}
